{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import ssl\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import clip\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Device selection\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"Using MPS\")\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "os.makedirs('output', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "def precompute_clip_embeddings(output_path):\n",
        "    print(\"Loading CLIP model...\")\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    # ImageFolder loads all images from the subfolders of the root directory\n",
        "    dataset = datasets.ImageFolder(\n",
        "        root='data/celeba_hq/img_align',\n",
        "        transform=preprocess\n",
        "    )\n",
        "\n",
        "    print(f\"Found {len(dataset)} images.\")\n",
        "    \n",
        "    batch_size_embed = 50 if device.type == 'cpu' else 100\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size_embed, shuffle=False)\n",
        "\n",
        "    all_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            image_features = model.encode_image(images)\n",
        "            all_embeddings.append(image_features.cpu())\n",
        "\n",
        "    embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    print(f\"Final embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "    torch.save({'embeddings': embeddings}, output_path)\n",
        "    print(f\"Saved all embeddings to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_path = 'celeba_hq_clip_embeddings.pt'\n",
        "\n",
        "if not os.path.exists(embeddings_path):\n",
        "    precompute_clip_embeddings(embeddings_path)\n",
        "else:\n",
        "    print(f\"Embeddings file already exists: {embeddings_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
