{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import ssl\n",
        "import os\n",
        "import clip\n",
        "import time\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Device selection\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"Using MPS\")\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if device.type == 'cuda' else {'num_workers': 0}\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 if device.type == 'cuda' else 8\n",
        "latent_size = 64\n",
        "clip_dim = None  # TODO: CLIP embeddings are ___-dimensional\n",
        "epochs = 20\n",
        "beta = 0.5\n",
        "init_channels = 32\n",
        "image_size = 64  # downsample CelebA to 64x64 for manageable training\n",
        "\n",
        "os.makedirs('output', exist_ok=True)\n",
        "\n",
        "# Logging to Tensorboard\n",
        "current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "run_name = f'runs/celeba_cvae_v1_{current_time}'\n",
        "writer = SummaryWriter(log_dir=run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CelebAWithCLIPEmbeddings(torch.utils.data.Dataset):\n",
        "    def __init__(self, split, embeddings_path, data_dir='./data'):\n",
        "        self.split = split\n",
        "        self.embeddings_path = embeddings_path\n",
        "        \n",
        "        self.celeba = datasets.CelebA(\n",
        "            root=data_dir, \n",
        "            split=split,  # 'train', 'valid', or 'test'\n",
        "            download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize((image_size, image_size)),  # Resize to manageable size\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        )\n",
        "        \n",
        "        self.embeddings_data = torch.load(embeddings_path, map_location='cpu')\n",
        "        self.embeddings = self.embeddings_data[f'{split}']\n",
        "        self.labels = self.embeddings_data[f'{split}_labels']  # These are 40 attributes\n",
        "        \n",
        "        print(f\"Loaded {len(self.embeddings)} CLIP embeddings for {split} set\")\n",
        "        print(f\"CelebA attributes shape: {self.labels.shape if hasattr(self.labels, 'shape') else 'N/A'}\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.celeba)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image, attributes = self.celeba[idx]  # attributes is tensor of shape [40]\n",
        "        clip_embedding = self.embeddings[idx]\n",
        "        return image, attributes, clip_embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CelebaCVAE(nn.Module):\n",
        "    def __init__(self, image_channels, init_channels, latent_size, class_size, image_size=64):\n",
        "        super(CelebaCVAE, self).__init__()\n",
        "        self.image_channels = image_channels\n",
        "        self.latent_size = latent_size\n",
        "        self.class_size = class_size  # clip_dim for CLIP embeddings\n",
        "        self.init_channels = init_channels\n",
        "        self.image_size = image_size\n",
        "        \n",
        "        conv_output_size = init_channels * 8  # Final channel count\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # 64x64 -> 32x32\n",
        "            nn.Conv2d(self.image_channels, init_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 32x32 -> 16x16\n",
        "            nn.Conv2d(init_channels, init_channels*2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 16x16 -> 8x8\n",
        "            nn.Conv2d(init_channels*2, init_channels*4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*4),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 8x8 -> 4x4\n",
        "            nn.Conv2d(init_channels*4, init_channels*8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*8),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 4x4 -> 2x2\n",
        "            nn.Conv2d(init_channels*8, init_channels*8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*8),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 2x2 -> 1x1\n",
        "            nn.Conv2d(init_channels*8, conv_output_size, kernel_size=2, stride=1, padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # FC layers to get mu and logvar\n",
        "        # TODO: These layers need to accept CLIP embeddings (self.class_size)\n",
        "        self.fc1 = nn.Linear(conv_output_size, 512)  # TODO: Modify\n",
        "        self.fc_mu = nn.Linear(512, self.latent_size)\n",
        "        self.fc_logvar = nn.Linear(512, self.latent_size)\n",
        "        self.fc2 = nn.Linear(self.latent_size, conv_output_size)  # TODO: Modify\n",
        "        \n",
        "        # Decoder for 64x64 images\n",
        "        self.decoder = nn.Sequential(\n",
        "            # 1x1 -> 2x2\n",
        "            nn.ConvTranspose2d(conv_output_size, init_channels*8, kernel_size=2, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(init_channels*8),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 2x2 -> 4x4\n",
        "            nn.ConvTranspose2d(init_channels*8, init_channels*8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*8),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 4x4 -> 8x8\n",
        "            nn.ConvTranspose2d(init_channels*8, init_channels*4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*4),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 8x8 -> 16x16\n",
        "            nn.ConvTranspose2d(init_channels*4, init_channels*2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels*2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 16x16 -> 32x32\n",
        "            nn.ConvTranspose2d(init_channels*2, init_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(init_channels),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 32x32 -> 64x64\n",
        "            nn.ConvTranspose2d(init_channels, self.image_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()  # output values between 0 and 1\n",
        "        )\n",
        "    \n",
        "    def encode(self, x, c):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        \n",
        "        inputs = torch.cat([h, c], 1)\n",
        "        \n",
        "        h_fc = F.relu(self.fc1(inputs))\n",
        "        mu = self.fc_mu(h_fc)\n",
        "        logvar = self.fc_logvar(h_fc)\n",
        "        return mu, logvar\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = mu + eps * std\n",
        "        return sample\n",
        "    \n",
        "    def decode(self, z, c):\n",
        "        inputs = torch.cat([z, c], 1)\n",
        "        \n",
        "        h = F.relu(self.fc2(inputs))\n",
        "        h = h.view(-1, self.init_channels * 8, 1, 1)\n",
        "        \n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        mu, logvar = self.encode(x, c)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(z, c)\n",
        "        return recon_x, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x.view(recon_x.size(0), -1), x.view(x.size(0), -1), reduction='sum')\n",
        "    KLD = -beta * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD * 0.5, BCE, KLD\n",
        "\n",
        "def train(epoch, model, optimizer, train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch_idx, (data, attributes, clip_embeddings) in enumerate(train_loader):\n",
        "        data, attributes, clip_embeddings = data.to(device), attributes.to(device), clip_embeddings.to(device)\n",
        "        \n",
        "        # TODO: Pass clip_embeddings to the model (unlike MNIST's one_hot encoding, use clip_embeddings directly)\n",
        "        recon_batch, mu, logvar = model(___, ___)  # TODO: Modify\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)[0]\n",
        "        loss.backward()\n",
        "        train_loss += loss.detach().cpu().numpy()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:  # less frequent logging for larger dataset\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "    \n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, avg_loss))\n",
        "\n",
        "    # Log to Tensorboard\n",
        "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
        "    \n",
        "def test(epoch, model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    total_bce = 0\n",
        "    total_kld = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (data, attributes, clip_embeddings) in enumerate(test_loader):\n",
        "            data, attributes, clip_embeddings = data.to(device), attributes.to(device), clip_embeddings.to(device)\n",
        "            \n",
        "            recon_batch, mu, logvar = model(data, clip_embeddings)\n",
        "            loss, bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "            \n",
        "            test_loss += loss.detach().cpu().numpy()\n",
        "            total_bce += bce.detach().cpu().numpy()\n",
        "            total_kld += kld.detach().cpu().numpy()\n",
        "            \n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)  # Show more samples for faces\n",
        "                comparison = torch.cat([data[:n], recon_batch[:n]])\n",
        "                grid = make_grid(comparison.cpu(), nrow=n, normalize=True)\n",
        "                writer.add_image('Reconstruction', grid, epoch)\n",
        "                # save_image(comparison.cpu(),\n",
        "                #          'output/reconstruction_' + str(f\"{epoch:02}\") + '.png', nrow=n)\n",
        "                save_image(comparison.cpu(),\n",
        "                         f'output/reconstruction_{epoch:02d}.png', nrow=n, normalize=True)\n",
        "    \n",
        "    avg_loss = test_loss / len(test_loader.dataset)\n",
        "    avg_bce = total_bce / len(test_loader.dataset)\n",
        "    avg_kld = total_kld / len(test_loader.dataset)\n",
        "    \n",
        "    print('====> Test set loss: {:.4f}'.format(avg_loss))\n",
        "    \n",
        "    # Log to Tensorboard\n",
        "    writer.add_scalar('Loss/test', avg_loss, epoch)\n",
        "    writer.add_scalar('Loss/BCE', avg_bce, epoch)\n",
        "    writer.add_scalar('Loss/KLD', avg_kld, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Specify path to pre-computed CLIP embeddings\n",
        "# Note: We ran setup_clip_embeddings.ipynb first to generate this file\n",
        "embeddings_path = None\n",
        "\n",
        "train_dataset = CelebAWithCLIPEmbeddings('train', embeddings_path, './data')\n",
        "valid_dataset = CelebAWithCLIPEmbeddings('valid', embeddings_path, './data')\n",
        "test_dataset = CelebAWithCLIPEmbeddings('test', embeddings_path, './data')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Valid: {len(valid_dataset)}, Test: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Set image_channels to account for RGB (unlike MNIST's 1 channel)\n",
        "image_channels = None\n",
        "\n",
        "model = CelebaCVAE(image_channels, init_channels, latent_size, clip_dim, image_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4, betas=(0.5, 0.999))  # Better params for faces\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch, model, optimizer, train_loader)\n",
        "    test(epoch, model, valid_loader)  # Use validation set for monitoring\n",
        "    \n",
        "    # Generate sample faces every few epochs\n",
        "    if epoch % 5 == 0:\n",
        "        with torch.no_grad():\n",
        "            # Sample random embeddings from test set\n",
        "            sample_embeddings = []\n",
        "            for i in range(8):\n",
        "                idx = torch.randint(0, len(test_dataset), (1,)).item()\n",
        "                sample_embeddings.append(test_dataset.embeddings[idx])\n",
        "            \n",
        "            # TODO: Stack embeddings and move to device\n",
        "            sample_clip_embeddings = None  # HINT: torch.stack(____).to(device)\n",
        "            sample_z = torch.randn(8, latent_size).to(device)\n",
        "            \n",
        "            # TODO: Decode using latent z and CLIP embeddings\n",
        "            sample = None  # HINT: model.decode(___, ___).cpu()\n",
        "            \n",
        "            # Log images to Tensorboard\n",
        "            sample_grid = make_grid(sample, nrow=4, normalize=True)\n",
        "            writer.add_image('Generated/from_embeddings', sample_grid, epoch)\n",
        "            \n",
        "            save_image(sample, f'output/sample_{epoch:02d}.png', nrow=4, normalize=True)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'celeba_cvae_model.pth')\n",
        "\n",
        "# Close the Tensorboard writer\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text prompt face generation\n",
        "\n",
        "def generate_faces_with_text(model, text_prompt, num_samples=8):\n",
        "    print(f\"Generating faces for text: '{text_prompt}'\")\n",
        "    \n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "    \n",
        "    text = clip.tokenize([text_prompt]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_z = torch.randn(num_samples, latent_size).to(device)\n",
        "        \n",
        "        # TODO: Repeat text features for all samples\n",
        "        text_condition = None  # Hint: text_features.repeat(___, 1)\n",
        "        \n",
        "        # TODO: Decode\n",
        "        sample = None  # HINT: model.decode(___, ___).cpu()\n",
        "        \n",
        "        filename = f'text_generation_{text_prompt.replace(\" \", \"_\").replace(\",\", \"\")}.png'\n",
        "        save_image(sample, filename, nrow=4, normalize=True)\n",
        "        \n",
        "        print(f\"Generated faces saved to {filename}\")\n",
        "        return sample\n",
        "\n",
        "# Generate faces for different prompts\n",
        "face_prompts = [\n",
        "    \"smiling woman\",\n",
        "    \"man with beard\", \n",
        "    \"young person\",\n",
        "    \"person with glasses\",\n",
        "    \"blonde hair\",\n",
        "    \"dark hair\"\n",
        "]\n",
        "\n",
        "for prompt in face_prompts:\n",
        "    generate_faces_with_text(model, prompt, 8)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
