{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device selection\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"Using MPS\")\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1}\n",
        "if torch.cuda.is_available():\n",
        "    kwargs['pin_memory'] = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "latent_size = 32\n",
        "init_channels = 8\n",
        "class_size = None  # TODO: parameter for the number of classes in MNIST (0-9)\n",
        "epochs = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        'data', train=True, download=True,\n",
        "        transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        'data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=False, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot(labels, class_size):\n",
        "    \"\"\"\n",
        "    Hint: Create a tensor of zeros with shape (batch_size, class_size)\n",
        "    Then set the appropriate indices to 1 based on the labels\n",
        "    \"\"\"\n",
        "    # TODO: Create a tensor of zeros with the correct shape\n",
        "    targets = None\n",
        "    \n",
        "    # TODO: Fill in the one-hot encoding\n",
        "    # For each sample i and its corresponding label, set corresponding target index to 1\n",
        "    for i, label in enumerate(labels):\n",
        "        pass\n",
        "    \n",
        "    return targets.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CVAE(nn.Module):\n",
        "    \"\"\"    \n",
        "    Key differences from regular VAE:\n",
        "    1. Constructor takes an additional 'class_size' parameter\n",
        "    2. encode() and decode() methods take an additional condition parameter 'c'\n",
        "    3. FC layers are modified to concatenate class information\n",
        "    \"\"\"\n",
        "    def __init__(self, image_channels, init_channels, latent_size, class_size):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.image_channels = image_channels\n",
        "        self.latent_size = latent_size\n",
        "        self.class_size = class_size\n",
        "        self.init_channels = init_channels\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, init_channels, kernel_size=3, stride=2, padding=1),     # (1, 28, 28) -> (8, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(init_channels, init_channels*2, kernel_size=3, stride=2, padding=1),    # (8, 14, 14) -> (16, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(init_channels*2, init_channels*4, kernel_size=3, stride=2, padding=1),  # (16, 7, 7) -> (32, 4, 4)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(init_channels*4, 64, kernel_size=3, stride=1, padding=0),               # (32, 4, 4) -> (64, 2, 2)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=2, stride=1, padding=0),                          # (64, 2, 2) -> (64, 1, 1)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Hint: The input size should be 64 + ___ for fc1, and latent_size + ___ for fc2\n",
        "        self.fc1 = nn.Linear(64, 128)  # TODO: Modify\n",
        "        self.fc_mu = nn.Linear(128, latent_size)\n",
        "        self.fc_logvar = nn.Linear(128, latent_size)\n",
        "        self.fc2 = nn.Linear(latent_size, 64)  # TODO: Modify\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=1, padding=0),  # (64, 1, 1) -> (64, 2, 2)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, init_channels*4, kernel_size=3, stride=1, padding=0),  # (64, 2, 2) -> (32, 4, 4)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(init_channels*4, init_channels*2, kernel_size=3, stride=2, padding=1),  # (32, 4, 4) -> (16, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(init_channels*2, init_channels, kernel_size=3, stride=2, padding=1, output_padding=1),  # (16, 7, 7) -> (8, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(init_channels, image_channels, kernel_size=4, stride=2, padding=1)  # (8, 14, 14) -> (1, 28, 28)\n",
        "        )\n",
        "    \n",
        "    # TODO: Modify the encode method to accept class condition 'c'\n",
        "    def encode(self, x, ):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        \n",
        "        # Hint: Use torch.cat(___, 1) to concatenate h and c along dimension 1\n",
        "        inputs = None  # TODO: Modify\n",
        "        \n",
        "        h_fc = F.relu(self.fc1(inputs))\n",
        "        mu = self.fc_mu(h_fc)\n",
        "        logvar = self.fc_logvar(h_fc)\n",
        "        return mu, logvar\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = mu + eps * std\n",
        "        return sample\n",
        "    \n",
        "    # TODO: Modify the decode method to accept class condition 'c'\n",
        "    def decode(self, z, ):\n",
        "        # Hint: Similar logic to encoder\n",
        "        inputs = None  # TODO: Modify\n",
        "        \n",
        "        h = F.relu(self.fc2(inputs))\n",
        "        h = h.view(-1, 64, 1, 1) \n",
        "        return torch.sigmoid(self.decoder(h))\n",
        "\n",
        "    # TODO: Modify the forward method to accept class condition 'c'\n",
        "    def forward(self, x, ):\n",
        "        mu, logvar = self.encode(x, ) # TODO: Modify\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(z, ) # TODO: Modify\n",
        "        return recon_x, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function - same as original VAE\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    BCE = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "    # -0.5 * torch.sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, epoch, losses):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        \n",
        "        # Hint: Use the one_hot function you implemented earlier\n",
        "        labels_onehot = None  # TODO: Modify\n",
        "        recon_batch, mu, logvar = model(data, labels_onehot)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.detach().cpu().numpy()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 20 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "    \n",
        "    losses.append(train_loss / len(train_loader.dataset))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(model, epoch, losses):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(test_loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            \n",
        "            # Hint: Same as train()\n",
        "            labels_onehot = None  # TODO: Modify\n",
        "            recon_batch, mu, logvar = model(data, labels_onehot) \n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).detach().cpu().numpy()\n",
        "            \n",
        "            # Save reconstruction comparison for first batch\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 5)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(-1, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         'cvae_reconstruction_' + str(f\"{epoch:02}\") + '.png', nrow=n)\n",
        "\n",
        "    losses.append(test_loss / len(test_loader.dataset))\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train the CVAE model\n",
        "model = CVAE(image_channels=1, init_channels=init_channels, latent_size=latent_size, class_size=class_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "print(\"Starting CVAE training...\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, optimizer, epoch, train_losses)\n",
        "    test(model, epoch, test_losses)\n",
        "    \n",
        "    # Generate samples for each class\n",
        "    with torch.no_grad():\n",
        "        # Hint: torch.eye(___, ___) creates an identity matrix (perfect for one-hot encoding)\n",
        "        c = None  # TODO: Replace with torch.eye(10, 10).to(device)\n",
        "        sample = torch.randn(10, latent_size).to(device)\n",
        "        sample = model.decode(sample, c).cpu()\n",
        "        save_image(sample.view(10, 1, 28, 28), \n",
        "                  str(f\"cvae_sample_{epoch:02}.png\"))\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the training and test losses\n",
        "print(\"Train losses:\", train_losses)\n",
        "print(\"Test losses:\", test_losses)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(test_losses, label='Test Loss', marker='s')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CVAE Training and Test Losses')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save and load model\n",
        "torch.save(model.state_dict(), 'cvae_model_mnist.pth')\n",
        "print(\"Model saved as 'cvae_model_mnist.pth'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model (skip training for future use)\n",
        "# model = CVAE(image_channels=1, init_channels=init_channels, latent_size=latent_size, class_size=class_size).to(device)\n",
        "# model.load_state_dict(torch.load('cvae_model_mnist.pth'))\n",
        "# print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_digit(model, digit, num_samples=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Hint: Create a tensor of zeros with shape (num_samples, class_size)\n",
        "        c = None  # TODO: Modify\n",
        "        c[:, digit] = 1        \n",
        "        # Hint: Sample from latent space using torch.randn(___, ___).to(device)\n",
        "        z = None  # TODO: Modify\n",
        "        \n",
        "        sample = None # TODO: Generate samples using the decode method with both z and c\n",
        "        save_image(sample.view(num_samples, 1, 28, 28), \n",
        "                  f\"cvae_generated_digit_{digit}.png\", nrow=5)\n",
        "        print(f\"Generated {num_samples} samples of digit {digit}\")\n",
        "\n",
        "def generate_all_digits(model, num_samples_per_digit=8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_samples = []\n",
        "        \n",
        "        for digit in range(10):\n",
        "            # TODO: Use logic from generate_digit() to define c, z, and sample\n",
        "            c = None\n",
        "            c[:, digit] = None\n",
        "            z = None\n",
        "\n",
        "            sample = None\n",
        "            all_samples.append(sample)\n",
        "        \n",
        "        all_samples = torch.cat(all_samples, dim=0)\n",
        "        save_image(all_samples.view(-1, 1, 28, 28), \n",
        "                  \"cvae_all_digits_generated.png\", nrow=num_samples_per_digit)\n",
        "        print(f\"Generated {num_samples_per_digit} samples for each digit (0-9)\")\n",
        "\n",
        "# TODO: Test your conditional generation functions\n",
        "print(\"Testing conditional generation...\")\n",
        "generate_digit(model, 3, num_samples=10)  # example: generate 10 threes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interpolate_between_classes(model, class1, class2, num_steps=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # TODO: Create one-hot encodings for both classes (replace None's)\n",
        "        c1 = None\n",
        "        c1[0, class1] = 1\n",
        "        c2 = None\n",
        "        c2[0, class2] = 1\n",
        "        \n",
        "        z1 = None\n",
        "        z2 = None\n",
        "        \n",
        "        interpolated_samples = []\n",
        "        \n",
        "        for i in range(num_steps):\n",
        "            # Linear interpolation parameter (0 to 1)\n",
        "            alpha = i / (num_steps - 1)\n",
        "            \n",
        "            # Interpolate in latent space\n",
        "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
        "            # Hint: use z_interp as reference for c_interp\n",
        "            c_interp = None  # TODO: Modify\n",
        "            \n",
        "            # Hint: use sample definitions from earlier\n",
        "            sample = None  # TODO: Generate sample with interpolated conditions \n",
        "            interpolated_samples.append(sample)\n",
        "        \n",
        "        interpolated_samples = torch.cat(interpolated_samples, dim=0)\n",
        "        save_image(interpolated_samples.view(-1, 1, 28, 28), \n",
        "                  f\"cvae_interpolation_{class1}_to_{class2}.png\", nrow=num_steps)\n",
        "        print(f\"Generated interpolation from class {class1} to class {class2}\")\n",
        "\n",
        "# TODO: Test interpolation\n",
        "interpolate_between_classes(model, 0, 1, num_steps=10)  # example: 0 to 1\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
